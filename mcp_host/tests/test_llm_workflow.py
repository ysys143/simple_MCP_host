#!/usr/bin/env python3
"""LLM ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

OpenAI ChatGPTë¥¼ í™œìš©í•œ ìì—°ì–´ ì´í•´ì™€ ì‘ë‹µ ìƒì„±ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
í‚¤ì›Œë“œ ë§¤ì¹­ì—ì„œ ì§„ì •í•œ AI ê¸°ë°˜ ëŒ€í™” ì‹œìŠ¤í…œìœ¼ë¡œì˜ ì—…ê·¸ë ˆì´ë“œë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import asyncio
import sys
import os
import logging
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from mcp_host.workflows import create_workflow_executor


# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


async def test_llm_workflow():
    """LLM ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸"""
    print("=== LLM ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ===")
    
    # OpenAI API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°±ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ê³„ì†í•©ë‹ˆë‹¤.")
    else:
        print("âœ… OpenAI API í‚¤ í™•ì¸ë¨")
    
    try:
        # 1. ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ê¸° ìƒì„±
        executor = create_workflow_executor()
        print("1. LLM ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ê¸° ìƒì„± ì™„ë£Œ")
        
        # 2. ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        test_cases = [
            {
                "input": "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ì„œìš¸ ë‚ ì”¨ê°€ ì–´ë–¤ê°€ìš”?",
                "expected_intent": "WEATHER_QUERY",
                "description": "ë‚ ì”¨ ì¡°íšŒ (ìì—°ì–´)"
            },
            {
                "input": "í˜„ì¬ ë””ë ‰í† ë¦¬ì— ìˆëŠ” íŒŒì¼ë“¤ì„ ë³´ì—¬ì£¼ì„¸ìš”",
                "expected_intent": "FILE_OPERATION", 
                "description": "íŒŒì¼ ëª©ë¡ ì¡°íšŒ (ìì—°ì–´)"
            },
            {
                "input": "MCP ì„œë²„ ìƒíƒœëŠ” ì–´ë–¤ê°€ìš”?",
                "expected_intent": "SERVER_STATUS",
                "description": "ì„œë²„ ìƒíƒœ í™•ì¸"
            },
            {
                "input": "ë„ì›€ë§ì„ ë³´ê³  ì‹¶ì–´ìš”",
                "expected_intent": "HELP",
                "description": "ë„ì›€ë§ ìš”ì²­"
            },
            {
                "input": "íŒŒì´ì¬ê³¼ ìë°”ìŠ¤í¬ë¦½íŠ¸ì˜ ì°¨ì´ì ì´ ë­ì£ ?",
                "expected_intent": "GENERAL_CHAT",
                "description": "ì¼ë°˜ ëŒ€í™”"
            }
        ]
        
        success_count = 0
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\n--- í…ŒìŠ¤íŠ¸ {i}: {test_case['description']} ---")
            print(f"ì…ë ¥: {test_case['input']}")
            
            try:
                # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
                result = await executor.execute_message(
                    user_message=test_case['input'],
                    session_id=f"test_session_{i}",
                    context={"test_mode": True}
                )
                
                print(f"ì„±ê³µ: {result['success']}")
                print(f"ì‘ë‹µ: {result['response'][:100]}...")
                if result.get('intent_type'):
                    print(f"ì˜ë„: {result['intent_type']}")
                
                if result['success']:
                    success_count += 1
                    print("âœ… í…ŒìŠ¤íŠ¸ í†µê³¼")
                else:
                    print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result.get('error')}")
                
            except Exception as e:
                print(f"âŒ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        
        print(f"\nğŸ¯ ê²°ê³¼: {success_count}/{len(test_cases)} í…ŒìŠ¤íŠ¸ í†µê³¼")
        
        if success_count == len(test_cases):
            print("ğŸ‰ ëª¨ë“  LLM ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        else:
            print("ğŸ’¥ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        
        return success_count == len(test_cases)
        
    except Exception as e:
        print(f"ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_llm_fallback():
    """LLM ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± í…ŒìŠ¤íŠ¸"""
    print("\n=== LLM í´ë°± ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸ ===")
    
    # ì„ì‹œë¡œ API í‚¤ ì œê±°
    original_key = os.environ.pop("OPENAI_API_KEY", None)
    
    try:
        executor = create_workflow_executor()
        
        result = await executor.execute_message(
            user_message="ë‚ ì”¨ ì–´ë•Œ?",
            session_id="fallback_test",
            context={"test_mode": True}
        )
        
        print(f"í´ë°± ê²°ê³¼: {result['success']}")
        print(f"ì‘ë‹µ: {result['response'][:100]}...")
        
        # API í‚¤ ë³µì›
        if original_key:
            os.environ["OPENAI_API_KEY"] = original_key
        
        print("âœ… í´ë°± ë©”ì»¤ë‹ˆì¦˜ ì •ìƒ ì‘ë™")
        return True
        
    except Exception as e:
        print(f"í´ë°± í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        # API í‚¤ ë³µì›
        if original_key:
            os.environ["OPENAI_API_KEY"] = original_key
        return False


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("LLM ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    # LLM ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
    llm_test = await test_llm_workflow()
    
    # í´ë°± ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸  
    fallback_test = await test_llm_fallback()
    
    success = llm_test and fallback_test
    
    if success:
        print("\nğŸ‰ ëª¨ë“  LLM í…ŒìŠ¤íŠ¸ í†µê³¼!")
        print("ì´ì œ ì§„ì •í•œ AI ê¸°ë°˜ ëŒ€í™” ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ë‹¤ìŒ ë‹¨ê³„: FastAPI ì„œë²„ì— í†µí•©í•˜ì—¬ ì›¹ ì¸í„°í˜ì´ìŠ¤ë¡œ í…ŒìŠ¤íŠ¸")
    else:
        print("\nâŒ ì¼ë¶€ LLM í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
    
    return success


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1) 