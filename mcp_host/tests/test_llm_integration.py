#!/usr/bin/env python3
"""LLM ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸

ì‹¤ì œ OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ LLM ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import asyncio
import os
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

async def test_llm_workflow():
    """LLM ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ì „ì²´ í…ŒìŠ¤íŠ¸"""
    print("ğŸ¤– LLM ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    try:
        from mcp_host.workflows.executor import create_workflow_executor
        from mcp_host.adapters.client import create_client
        from mcp_host.config import create_config_manager
        
        # MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        print("1. MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”...")
        config_manager = create_config_manager()
        client = create_client(config_manager)
        await client.initialize("mcp_servers.json")
        print(f"âœ… MCP í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ: {client.get_server_count()}ê°œ ì„œë²„, {len(client.get_tools())}ê°œ ë„êµ¬")
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ê¸° ìƒì„±
        print("\n2. LLM í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš° ìƒì„±...")
        executor = create_workflow_executor()
        print("âœ… ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ê¸° ì¤€ë¹„ ì™„ë£Œ")
        
        # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ë“¤
        test_cases = [
            {
                "name": "ğŸŒ¤ï¸ ìì—°ì–´ ë‚ ì”¨ ì§ˆë¬¸",
                "input": "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ì„œìš¸ ë‚ ì”¨ê°€ ì–´ë–¤ê°€ìš”?",
                "expected_intent": "WEATHER_QUERY"
            },
            {
                "name": "ğŸ“ ìì—°ì–´ íŒŒì¼ ì§ˆë¬¸", 
                "input": "í˜„ì¬ ë””ë ‰í† ë¦¬ì— ìˆëŠ” íŒŒì¼ë“¤ì„ ë³´ì—¬ì£¼ì„¸ìš”",
                "expected_intent": "FILE_OPERATION"
            },
            {
                "name": "ğŸ’¬ ì¼ë°˜ ëŒ€í™”",
                "input": "íŒŒì´ì¬ê³¼ ìë°”ìŠ¤í¬ë¦½íŠ¸ì˜ ì°¨ì´ì ì´ ë­”ê°€ìš”?",
                "expected_intent": "GENERAL_CHAT"
            },
            {
                "name": "ğŸŒ§ï¸ êµ¬ì²´ì ì¸ ì˜ˆë³´ ìš”ì²­",
                "input": "ë¶€ì‚° 3ì¼ ë‚ ì”¨ ì˜ˆë³´ ì•Œë ¤ì£¼ì„¸ìš”",
                "expected_intent": "WEATHER_QUERY"
            }
        ]
        
        print(f"\n3. {len(test_cases)}ê°œ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰...")
        print("-" * 50)
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\n[í…ŒìŠ¤íŠ¸ {i}] {test_case['name']}")
            print(f"ì…ë ¥: {test_case['input']}")
            
            try:
                result = await executor.execute_message(
                    user_message=test_case['input'],
                    session_id=f"test_{i}",
                    mcp_client=client
                )
                
                if result.get("success"):
                    print(f"âœ… ì„±ê³µ: {result.get('intent_type', 'Unknown')}")
                    response = result.get('response', 'No response')
                    print(f"ì‘ë‹µ: {response[:100]}..." if len(response) > 100 else f"ì‘ë‹µ: {response}")
                    if result.get('tool_calls'):
                        print(f"ë„êµ¬ í˜¸ì¶œ: {len(result['tool_calls'])}ê°œ")
                        for tool_call in result['tool_calls']:
                            print(f"  - {tool_call['server']}.{tool_call['tool']}")
                else:
                    print(f"âŒ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
                    
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
            
            print("-" * 30)
        
        print("\n4. LLM vs í‚¤ì›Œë“œ ë¹„êµ í…ŒìŠ¤íŠ¸...")
        
        # ê°™ì€ ì§ˆë¬¸ì„ LLMê³¼ í‚¤ì›Œë“œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•´ë³´ê¸°
        test_input = "ì„œìš¸ ë‚ ì”¨"
        print(f"ì…ë ¥: {test_input}")
        
        try:
            result = await executor.execute_message(
                user_message=test_input,
                session_id="comparison_test",
                mcp_client=client
            )
            
            response = result.get('response', '')
            print(f"ê²°ê³¼: {result.get('intent_type')} - {response[:80]}..." if len(response) > 80 else f"ê²°ê³¼: {result.get('intent_type')} - {response}")
            
        except Exception as e:
            print(f"âŒ ë¹„êµ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        
        # ì •ë¦¬
        await client.close()
        print("\nğŸ‰ LLM í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # OpenAI API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        exit(1)
    
    print(f"ğŸ”‘ OpenAI API í‚¤: {os.getenv('OPENAI_API_KEY')[:10]}...")
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_llm_workflow()) 